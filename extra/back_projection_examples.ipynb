{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fancy-instrument",
   "metadata": {},
   "source": [
    "# How to perform back projection of feature weights?\n",
    "\n",
    "One of the potential issues of employing high dimensional parcellations is they can become difficult to interpret. While it would be great if there were existing high dimensional easy to interpret parcellations, this is not the case in practice. Instead, we propose that feature importances generated from parcellations that are not well suited to being discussed easily, (e.g., randomly generated or parcellations with thousands of regions), be back projected onto their original surface representation. Once represented at the vertex / surface level, it should be possible for researchers to interpret their findings as there exists an extensive literature of results presented and interpreted in the standard space. One could even re-parcellate results into a familiar anatomical atlas if desired.\n",
    "\n",
    "This example covers the back projection of feature weights to native surface space for a number of different pipeline / parcellation pairs explored in the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../exp/\")\n",
    "\n",
    "from models import get_pipe\n",
    "import BPt as bp\n",
    "import numpy as np\n",
    "import os\n",
    "from neurotools.plotting import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-quantity",
   "metadata": {},
   "source": [
    "We use the already saved dataset, but with one tweak to make out lives a little easier. Instead of using the consolidated data files, we load each seperately so that they are easier to plot and keep track of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "data = bp.read_pickle('../data/dataset.pkl')\n",
    "\n",
    "# Let's replace the loaded consolidated data files\n",
    "data = bp.Dataset(data['target'])\n",
    "data = data.set_role(scope=list(data), role='target')\n",
    "data.shape\n",
    "\n",
    "# Add data files\n",
    "files = {'curv': '../data/abcd_structural/curv/*.npy',\n",
    "         'sulc': '../data/abcd_structural/sulc/*.npy',\n",
    "         'thick': '../data/abcd_structural/thick/*.npy',\n",
    "         'myelin': '../data/abcd_structural/myelin/*.npy'}\n",
    "data = data.add_data_files(files, file_to_subject='auto')\n",
    "\n",
    "# Drop all data w/ all missing target\n",
    "data = data.drop_subjects_by_nan('target', threshold=.99)\n",
    "\n",
    "data.get_cols('data'), data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-literature",
   "metadata": {},
   "source": [
    "Next, we setup some common varaibles, as well as wrap the evaluate function in a helper function, and put the code used to plot the average inverse transformed feature weights for each modality.\n",
    "\n",
    "In this example, we will we predicting the regression target variable - 'anthro_waist_cm'. That said, all of the above code is designed to work with binary based targets as well (if running the code yourself, you can try changing the variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set problem spec\n",
    "ps = bp.ProblemSpec(target='anthro_waist_cm',\n",
    "                    random_state=5)\n",
    "\n",
    "# Helper function around eval\n",
    "def run_eval(pipe, n_jobs=8):\n",
    "    \n",
    "    # Run evaluate\n",
    "    return bp.evaluate(pipeline=pipe,\n",
    "                       dataset=data,\n",
    "                       problem_spec=ps,\n",
    "                       subjects='all',\n",
    "                       n_jobs=n_jobs,\n",
    "                       mute_warnings=True,\n",
    "                       cv=5)\n",
    "\n",
    "def plot_avg(inverse_fis):\n",
    "    \n",
    "    # Get average\n",
    "    avg_inverse = np.mean(np.array(inverse_fis), axis=0)\n",
    "    \n",
    "    # Set colorbar max and min same across each\n",
    "    # m = np.max(np.abs(avg_inverse))\n",
    "    # m *= 1.05\n",
    "    \n",
    "    # Use mean val as threshold\n",
    "    thresh = np.mean(np.mean(np.abs(avg_inverse)))\n",
    "    \n",
    "    # Plot average features for each modality seperate\n",
    "    for name, vals in zip(inverse_fis[0].index, avg_inverse):\n",
    "        plot(vals, threshold=thresh, title=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-contributor",
   "metadata": {},
   "source": [
    "## Base Elastic-Net\n",
    "\n",
    "The first example we will look at is simply back projecting the weights from an elastic-net based pipeline from one of the randomly generated parcellations. Since this regressor generates beta-weights, it is relatively simple to back project this values according to the values of the parcellation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipelines\n",
    "pipe = get_pipe('elastic', 'random_50_0')\n",
    "\n",
    "# Run evaluate\n",
    "results = run_eval(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-chart",
   "metadata": {},
   "source": [
    "The back projected feature importancescan be obtained with special BPt function `get_inverse_fis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_fis = results.get_inverse_fis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-jaguar",
   "metadata": {},
   "source": [
    "Looking a little closer, we see this returns us a list of pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inverse_fis), inverse_fis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-irrigation",
   "metadata": {},
   "source": [
    "We can further see that the first fold, first modality has the correct shape / number of values per vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_fis[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-miller",
   "metadata": {},
   "source": [
    "Now, if we are interested in plotting, we can generate average values across all 5 folds to plot as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_inverse = np.mean(np.array(inverse_fis), axis=0)\n",
    "avg_inverse.shape, avg_inverse[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-knowing",
   "metadata": {},
   "source": [
    "This logic is already wrapped up in the plotting function, let's try it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-prison",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-isolation",
   "metadata": {},
   "source": [
    "## Base - LGBM\n",
    "\n",
    "As out next example, we are just using an LGBM based pipeline instead of the elastic-net one. In this case we just repeat the same steps, except this time we are plotting the automatically computed LGBM based metrics of feature importance, instead of beta weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-incidence",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get pipelines\n",
    "pipe = get_pipe('lgbm', 'random_50_0')\n",
    "\n",
    "# Run evaluate\n",
    "results = run_eval(pipe)\n",
    "\n",
    "inverse_fis = results.get_inverse_fis()\n",
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-lecture",
   "metadata": {},
   "source": [
    "## What about a voting ensemble of Elastic-Net ?\n",
    "\n",
    "In this case, what goes on behind the scenes is that the coef_ from each of the base models are averaged. In a more detailed sense, when get inverse fis is called, the coef_ from each base ensemble are first back projected to the original space, then they are averaged. Even though each base estimator has 100 coef_, it would be wrong to average them in that space since each coef_ refers to a different parcellation, it is therefore neccisary to average only after back projection. This is taken care of internally since the nested estimators of the voting ensemble have Loader objects. Let's try it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-carnival",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get pipeline\n",
    "pipe = get_pipe('elastic', 'voted_random_100_10_1')\n",
    "\n",
    "# Run evaluate\n",
    "results = run_eval(pipe)\n",
    "\n",
    "# Get inverse and plot\n",
    "inverse_fis = results.get_inverse_fis()\n",
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-interaction",
   "metadata": {},
   "source": [
    "This method is also dynamic enough to support averaging across parcellations of different sizes / with lgbm feature importances instead of coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipeline\n",
    "pipe = get_pipe('lgbm', 'voted_random_100-1000_3_0')\n",
    "\n",
    "# Run evaluate\n",
    "results = run_eval(pipe)\n",
    "\n",
    "# Inverse and plot\n",
    "results.get_inverse_fis()\n",
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-alabama",
   "metadata": {},
   "source": [
    "## SVM / Permutation Feature Importance\n",
    "\n",
    "For the Elastic-Net and LGBM, they both have default already calculated feature importances which we can back project and plot right away. For the SVM based pipelines though, this isn't possible. Instead we need to calculate feature importances in another way, in this example we will use permutation based feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipelines\n",
    "pipe = get_pipe('svm', 'random_50_0')\n",
    "\n",
    "# Run evaluate\n",
    "results = run_eval(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p_fis\n",
    "p_fis = results.permutation_importance(dataset=data,\n",
    "                                       n_repeats=10,\n",
    "                                       just_model=True,\n",
    "                                       nested_model=True,\n",
    "                                       return_as='dfs',\n",
    "                                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From permutation fis can get inverse by passing alongs means\n",
    "inverse_fis = results.get_inverse_fis(p_fis['importances_mean'])\n",
    "\n",
    "# Plot\n",
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-bidder",
   "metadata": {},
   "source": [
    "## SVM based voting ensemble\n",
    "\n",
    "So the way we had the voting ensemble feature importances set up before, we took the average of each once back projected. Now what about when we have an ensemble of SVM classifiers? In this case we need to do the permutation feature importances again, but we want to ensure that the features being permuted are the fully transformed features. Let's get an example going before getting into more details..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipeline\n",
    "pipe = get_pipe('svm', 'voted_random_100_2_0')\n",
    "\n",
    "# Run evaluate\n",
    "results = run_eval(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-pizza",
   "metadata": {},
   "source": [
    "So what happens internally when we are going to call `permutation_importance` is the following internal function is called to setup the proper X_val, and locate the proper sub estimator for each fold. In this case we want just just_model and nested_model to be True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base X and y from dataset\n",
    "X, y = data.get_Xy(results.ps)\n",
    "\n",
    "# Call this internal methods for fold 0\n",
    "estimator, X_val, y_val, feat_names =\\\n",
    "                results._get_val_fold_Xy(results.estimators[0],\n",
    "                                         X_df=X, y_df=y,\n",
    "                                         fold=0,\n",
    "                                         just_model=True,\n",
    "                                         nested_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-skill",
   "metadata": {},
   "source": [
    "Looking at X_val, we notice the shape might look a little funny - but we can confirm that it lines up with the feat names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape, len(feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-anchor",
   "metadata": {},
   "source": [
    "So what's going on here? First note that we have a random ensemble with two random parcellations each with size 100, and we also have 4 modalities. So that means after being loaded and transformed, each parcellation will yield  400 features, then two sub SVM models with front-end feature selection will be fed in 400 features each. So why do we have 598 features here? We can get a better idea of what these features represent if we look at the feat names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names[300:310], feat_names[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-civilian",
   "metadata": {},
   "source": [
    "So two things are going on here, the first you'll note is that there are some features missing. That is because these feat names and X_val represent the transformed data after any features that were selected to be removed by  the feature selection step were already removed. The other piece you'll note is the 0_ appended to the features above, and the '1_' appended to the last feature we printed. That is to say the features and X_val represent the concatenated fully transformed output from each of the two sub-svm models.\n",
    "\n",
    "Basically, this is exactly what we want, as we need all avaliable features to be already present (i.e., not waiting to be transformed) when we pass them to the function responsible for calculating the permutation based feature importances. Essentially the last step that happens internally, that we don't need to worry about, is the 'predict' function for BPtVotingEstimator is designed to automatically detect this alternative output, we can confirm that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimator despite still containing the nested Loaders\n",
    "# is designed to still be able to accept the already transformed\n",
    "# by its own nested Loader's input to its predict function\n",
    "estimator.predict(X_val).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-specific",
   "metadata": {},
   "source": [
    "Now that we know whats going on behind the scenes, we with BPt can just do the same exact thing as before, and it will take care of the details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p_fis\n",
    "p_fis = results.permutation_importance(dataset=data,\n",
    "                                       n_repeats=10,\n",
    "                                       just_model=True,\n",
    "                                       nested_model=True,\n",
    "                                       return_as='dfs',\n",
    "                                       n_jobs=-1)\n",
    "\n",
    "# Get inverse, same as before\n",
    "inverse_fis = results.get_inverse_fis(p_fis['importances_mean'])\n",
    "\n",
    "# Plot\n",
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-convenience",
   "metadata": {},
   "source": [
    "## Stacked Ensembles - Elastic-Net\n",
    "\n",
    "Okay, now what about with a stacking based ensemble? Well, we can do something simmilar to voting, but instead of just taking the mean of the existing feature importances, we want to take a weighted average according to the feature importances of the stacking regressor itself. Though note that in order for this to work we have to make a simplification, that being that we throw away information on magnitude, and instead consider only the absolute average values according to the absolute weights of the stacker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pipeline\n",
    "pipe = get_pipe('elastic', 'stacked_random_100_3_1')\n",
    "\n",
    "# Run evaluate\n",
    "results = run_eval(pipe)\n",
    "\n",
    "# Plot inverse\n",
    "results.get_inverse_fis()\n",
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-retro",
   "metadata": {},
   "source": [
    "We can also in the same manner as before consider the generation and plotting of permutation based feature importances, e.g., in the case of using an SVM based classifier instead of elastic-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p_fis\n",
    "p_fis = results.permutation_importance(dataset=data,\n",
    "                                       n_repeats=10,\n",
    "                                       just_model=True,\n",
    "                                       nested_model=True,\n",
    "                                       return_as='dfs',\n",
    "                                       n_jobs=-1)\n",
    "\n",
    "# Get inverse, same as before\n",
    "inverse_fis = results.get_inverse_fis(p_fis['importances_mean'])\n",
    "\n",
    "# Plot\n",
    "plot_avg(inverse_fis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-learning",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('bpt': conda)",
   "language": "python",
   "name": "python391jvsc74a57bd0816e2859f723fb77ad3214da0fbda681e8d4db93bd8b118618b521c0b1f5f48f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
