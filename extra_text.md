Machine Learning for Neuroimaging

Machine learning, and more broadly, prediction as a general framework, offers a number of inherent benefits, but it is not without its downsides. Importantly, it is difficult to assess if a specific machine learning workflow is optimal, as there exists a seemingly endless pool of choices for different classifiers, data pre-processing steps, ensemble strategies, and other choices. Likewise, due to the nature of predictive performance, it is difficult to claim that a proposed pipeline is optimal, as there is likely a better solution or set of steps which would yield better predictive performance. 

To complicate matters, it is further not the case that maximum or optimal performance is always desired. A few of the reasons for this are: 

1. The researcher may desire the end result to be easily interpretable. In many cases state of the art predictive models may require sophisticated ensembles over large numbers of different models, where the end result may be highly predictive, but difficult to interpret. Recent advances in deep learning based approaches have also seen high predictive performance across a wide range of domains, but can suffer from being difficult to interpret. That said, in both cases it is important to note that there is a great deal of work being done to develop new strategies for integrating the outputs of complex models (Samek 2020, Lundberg 2017)

2. The researcher may not have sufficient subjects for a thorough pipeline exploration. With a relatively small number of subjects, evaluating machine learning methods with techniques like cross-validation can lead to noisy estimates of performance with large error bars (Varoquaux 2018). Due to this inherent instability, the act of “exploring” different pipeline configurations (e.g., across different choices of model or parcellation) can overfit to the dataset despite use of cross validation. So while pipeline exploration may be a reasonable choice to improve out of sample performance with sufficient subjects, it may serve to actually decrease generalizability when this condition is not met (Arbabshirani, 2017). 

3. The researcher may not have access to the required computational resources. Especially in neuroimaging, the size of the raw data per subject can be immense with anywhere from hundreds of thousands of raw features (e.g., sMRI) to millions (e.g., fMRI time-series). As a number of ML methods either scale poorly with the number of features (e.g., support vector machines) or require a comparable number of subjects when the feature space is large (e.g., convolutional neural networks) data reduction may be necessary in order to reduce the computational burden to a practical level (Amari 1999, Goodfellow 2016). Ensembling methods, where typically large numbers of models are trained on the same data and then their predictions combined via averaging or a meta-model, can similarly serve to improve performance at the expense of increased computational resources (Dietterich, 2000).

Arbabshirani, M. R., Plis, S., Sui, J., & Calhoun, V. D. (2017). Single subject prediction of brain disorders in neuroimaging: Promises and pitfalls. Neuroimage, 145, 137-165.

Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). Deep learning (Vol. 1, No. 2). Cambridge: MIT press.

Samek, W., Montavon, G., Lapuschkin, S., Anders, C. J., & Müller, K. R. (2020). Toward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond. arXiv preprint arXiv:2003.07631.

Varoquaux, G. (2018). Cross-validation failure: small sample sizes lead to large error bars. Neuroimage, 180, 68-77.



------------------------------


The Figure
--------------
Figure 10 shows a highly simplified example where we assume there are two ground truth ‘real’ regions, R1 and R2, represented by solid line boxes on the top half of the figure. We also consider an individualized parcellation variant on the bottom figure where there are still two ground truth regions R1 and R2, but these regions vary slightly between two subjects, S1 and S2. The columns in this figure consider splitting 4 different sets of parcellations, where each parcel is represented by a dotted box and a shade of gray. The first parcellation (column 1), shows a parcellation with just one parcel. The next two cases, (columns 2 and 3), show two cases for parcellations with two parcels, and the last case, (column 4), shows a parcellation with three equally sized parcels.

To help provide some intuition around why increasing spatial parcellations resolution improves downstream performance, we consider a simplified example in Figure 10. Column 1 in both cases represents the case of too coarse spatial resolution, where two meaningful areas with separate information R1 and R2 get averaged together. Columns 2 and 3 are interesting in that they represent the case where the number of parcellations match the baseline ground truth number of regions. In this case we see in A3 that we are able to extract the maximum usable information in the case that the parcellations are exact, but see in A2 that if they are not exact then the parcel with R1 and half of R2 will contain partially corrupted information. On the other hand, if we consider the individualized parcellation ground truth, then we see that there is no way to match the ground truth with just two parcels, and that both cases like in A2 only allow capturing all usable information from one of the two ground truth regions. Lastly, in column 4, we see that by adding an additional parcellation where are able to in both cases (rows A and B) capture R1 and R2 without information from the other ‘leaking’ in. This is possible because a third parcellation is able to capture the overlap which can then be either ignored by the classifier or potentially exploited in the case that the overlap is meaningful. We additionally make the assumption here that parcels, for example the bottom parcellation in A4, can arrive at the correct mean value for the region without covering the full region. Notably if we were to continue the thought experiment further, considering parcellations of 4, 5, 6, ect… parcels, then this assumption very well might not hold, as it is likely not the case that every vertex in each ground truth region has the exact same value. 

Worst / Best Case Performance
------------------------------------

Figure 5 shows the worst (maximum rank across all target variables) and best (minimum rank across all target variables) case performance as averaged across results computed separately for each choice of model pipeline.

The top half of figure 5 considers the maximum rank that a parcellation scores across all of the considered target variables as a measure of worst case performance. Parcellations with a lower maximum rank can therefore be considered more reliable under this metric. Importantly, the parcellations with the highest score don’t follow the same close pattern between performance and size seen in earlier figures as well as the best case performance plot in the bottom of Figure 5. Instead, we observe the parcellations with the best worst case performance have ~400 and ~1000 parcels. The bottom of Figure 5 instead considers the lowest rank or best case performance. In this case, we see that most parcellations with over 1000 parcels saturate close to the lowest possible rank. We also note that parcellations with a hundred or less in some cases obtain a low score, but in general are unable to stay competitive relative to higher parcel parcellations. 
