---
layout: default
title: Intro to Results
---

## Mean Rank

The measure of performance presented within with project is typically in terms of 'Mean Rank'. This measure can be
potentially confusing as it can change subtly from figure to figure. In the most simple case though, Mean Rank, is 
as the name suggests, just an average of a parcellations 'ranks', where a parcellations rank is generated by comparing its
performance to other parcellations (when evaluated with the same ML Pipeline and for the same target variable). For example
consider the highly simplified example below:

![simple intro figure](https://raw.githubusercontent.com/sahahn/parc_scaling/master/data/intro_figure1.png)

In this case, rankings are computed across three different parcellations, each which have just a generic "score"
(which in the rest of the project depends on if the target variable is binary or regression,
but in both cases higher is better). Then rankings are assigned individually for each of two target variables,
here just Target 1 and 2, where the parcellation with the highest score gets Rank 1, then the parcellation with the next highest
score gets rank 2 and so on. Mean Rank can then be computed across in this case the separate ranks for each of the
two target variables. Or for example, Mean Rank could be computed both across different [Target Variables](./variables.html)
and / or across different [ML Pipelines](./ml_pipelines.html), depending on the specific figure or subset of results.

A key benefit of Mean Rank over employing metrics like explained variance directly is that we can now compare across both
different binary and regression metrics, as well as to address [scaling issues](./scaling_issues.html) between metrics
(e.g., [Sex at Birth](./target_variables#sex-at-birth) is more predictive than
[KSADS ADHD Composite](./target_variables#ksads-adhd-composite)). In some degenerative cases Mean Rank has the potential to hide
information about magnitude of difference, but in general when computed over a sufficient number of comparisons (in this case
different parcellations) then this will be less of an issue. For example consider that if two parcellations varied only by
a very slight amount in performance, then their mean rank as computed across many target variables would end up being very close,
as each each evaluated target will be noisy. On the other hand, if a parcellation outperforms another by a large amount across
a large number of target variables, then this will be accurately reflected as the better parcellation obtaining a much better
Mean Rank. The core idea here is that if the difference in performance is two small between two parcellation, i.e., not actually better,
then Mean Rank if computed across enough individual rankings will show the two parcellations to be equal.

## Modelling Results

We employ ordinary least squares regression (OLS), as implemented in the python package [statsmodel](https://www.statsmodels.org/stable/index.html)
to model results from the base experiments. Base notation for OLS equations are written in the R formula style as `A ~ B + C`
where `A` is the dependent variable and `B + C` are independent fixed effects.
Alternatively, if written as `A ~ B * D` then `D` will be added as a fixed effect
along with an interaction term between `B` and `D` (equivalent to alternate notation `A ~ B + D + B * D`).
If a fixed effect is categorical, then it is dummy coded and each dummy variable added as a fixed effect.
Lastly, if a variable is wrapped in `log10()`, then the logarithm of the variable with base 10 has been used.


## Elastic-Net Example

Next, we can consider an example plot showing real results from the core project experiment. In
this example we will look at only results from the [Elastic-Net](./ml_pipelines#elastic-net.html) based pipeline
and from [randomly generated parcellations](./parcellations#random-parcellations).

![Simple Example](https://raw.githubusercontent.com/sahahn/parc_scaling/master/analyze/Figures/simple_example.png)

The x-axis here represents the number of parcels that each parcellation has, and the y-axis, the [mean rank](./results_intro#mean-rank) as
computed by ranking each parcellation relative to all others plotted here for all 45 [target variables](./variables.html).

Another useful way to view results is to re-create the same plot, but on a log-log scale.

![Simple Example Log](https://raw.githubusercontent.com/sahahn/parc_scaling/master/analyze/Figures/simple_example_log.png)

In this case we start to see a more clear traditional linear pattern emerge.
In order to more formally [model](./results_intro#modelling-results) these results, we will first [estimate the region where
a powerlaw holds](./estimate_powerlaw.html), then on this subset of data points (sizes 10-1500),
fit a linear model as `log10(Mean_Rank) ~ log10(Size)`. See OLS fit summary:

{% include stats_example.html %}

We can also easily visualize the OLS fit onto the plot from before:

![With fit](https://raw.githubusercontent.com/sahahn/parc_scaling/master/analyze/Figures/simple_example_log_with_fit2.png)

## ML Pipeline Average Example

Lastly, we will update our definition of mean rank by now averaging across not just the 45 [Target Variables](./variables.html), but also
now across all three choices of [ML Pipelines](./ml_pipelines.html).
Each plotted point below is now averaged from 135 different individually computed ranks.

![All Example Log](https://raw.githubusercontent.com/sahahn/parc_scaling/master/analyze/Figures/all_example_log.png)

We can see here that by averaging now over pipeline too the linear pattern on the log-log plot becomes cleaner, and extends furthers.
This is due to [differences in performance across choice of pipeline](./by_pipeline.html).

## Model Choice of Parcellation Example

We can also add another layer of complexity, that is looking at more than one type of parcellation. We add in addition to
[random parcellations](./parcellations#random-parcellations), [existing parcellations](./parcellations#existing-parcellations).

![2 Types](https://raw.githubusercontent.com/sahahn/parc_scaling/master/analyze/Figures/all_example_log_2parcs.png)

This also opens up another chance for modelling our results, where this time we can try and account for type of parcellation. First
let's model the results with type of parcellation as fixed effect, specifically: `log10(Mean_Rank) ~ log10(Size) + Parcellation_Type`

{% include stats_example2.html %}

![All with fit](https://raw.githubusercontent.com/sahahn/parc_scaling/master/analyze/Figures/all_with_fit_example.png)
