---
layout: default
---

# Multiple Parcellation Strategies Setup

In addition to the [base analysis](./index#base-experiment-setup), we sought to quantify how additional strategies operating across multiple parcellations might perform. Given a potentially limitless number of potential configurations, we explored only a small subset. We ultimately tested 3 different strategies: [Grid](./multiple_parcellations_setup#grid), [Voted](./multiple_parcellations_setup#voted) and [Stacked](./multiple_parcellations_setup#stacked), which we explain later in more detail

## Research Goals

Specific questions we sought to address included which multiple parcellation strategy, as well as which parcellations are included in that strategy, and how those choices influence performance. For example, how do the number of parcellations as well as the number of parcels in each parcellations contribute to performance gains. Should the included parcellations for any one ensemble be all of one fixed size or instead span across different sizes (e.g., five parcellations of size 300 each versus five parcellations with sizes 100, 200, 300, 400 and 500). Finally, how these different decisions influence [trade-offs](./trade_offs.md) between performance, runtime and interpretability, is an important consideration.


## Parcellations Used

In all of the multiple parcellation based analytic approaches, [random parcellations](./parcellations#random-parcellations) were used as the source or "pool" of parcellations in which these strategies had access to. This choice was made as a virtually limitless number of [random parcellations](./parcellations#random-parcellations) can be generated at any desired spatial scale. This flexibility makes them ideal for testing the different [research goals](./multiple_parcellations_setup#research-goals) of interest here.


In order to treat choice of parcellation as a hyperparameter, we employed a nested grid search. A three-fold nested cross-validation scheme on the training set, respecting family structure as before (i.e., assigning members of the same family to the same fold), was used to evaluate each potential parcellation. Within each of these internal folds a ML pipeline was trained, with its own nested parameter tuning, and then evaluated on its respective internal validation set. This process yielded an average for each of the three folds’ scores for each parcellation. The parcellation which obtained the highest score was selected for re-training on the full training set which involved, as in each nested fold, training a ML pipeline with its own nested parameter search. The final trained ML estimator, with the selected best parcellation, was then used to evaluate the validation fold. This process was repeated across the whole training set according to the same five-fold structure as used in the base analyses, thus allowing the results to be directly comparable.

In ensemble analyses, we tested two different ensemble strategies: voting and stacking. In the voting ensemble approach, a separate estimator was trained for each available parcellation, where each individual pipeline-parcellation pair was trained in the same way as in the base analysis. To do this, first each trained ML pipeline from the previous step generated a prediction. Then, the voting ensemble aggregated the predictions as either the mean, in the case of regression, or the most frequently predicted class, in the case of classification. The aggregated scores were then scored as a single set of predictions. 

The stacking ensemble, while similar to the voting ensemble, is more complex. For each of the pipeline-parcellation combinations, a separate three-fold cross-validation framework was used in the training set. In this framework, three ML pipelines were trained on 2/3 of the training set and predictions were made on the remaining 1/3, yielding an out-of-sample prediction for each participant in the training set. The predictions from all pipeline-parcellation combinations were used as features to train a “stacking model”. The purpose of the stacking model was to learn a relative weighting of each parcellation-pipeline combination (i.e., to give more weight to better parcellation-pipeline combinations and less weight to worse ones). The algorithm used to train the stacking model was a ridge penalized linear or logistic regression with nested hyper-parameter tuning. Once trained, this stacking model was used to predict the target variable in a novel sample (i.e., the held-out test set). The stacking ensemble procedure notably involved a large increase in computation relative to the voting ensemble, as the stacking ensemble involved training three pipelines for each parcellation-pipeline combination, whereas the voting ensemble consisted of training only one ML pipeline for each.


## Evaluation 

Each considered multiple parcellation strategy, Grid (choice of parcellation as a hyper-parameter), Voted (voting ensemble) and Stacked (stacking ensemble) was evaluated in a directly comparable way to the base analysis (i.e., for each target with the same five-fold cross-validation). As in the base analysis, multiple parcellation analyses were first run for each choice of ML pipeline separately. Additionally, we also considered ensembling and selecting across both parcellation and choice of ML pipeline (e.g., a voting ensemble which averages predictions from SVM, Elastic-Net and LGBM pipelines, each trained on random parcellations of size 100, 200 and 300). 

For the number of different parcellations available to a search or ensemble strategy, we evaluated four different numbers of parcellations: 3, 5, 8 and 10. Further, for each of these numbers of parcellations, we tested fixed size parcellations as well as differentially sized parcellations across a range of sizes (100, 200, 300, 400, 500, 50-500, 100-1000 and 300-1200). For example, for a combination of 3 parcellations and a fixed size of 100, three random parcellations with size 100 could be used. For a combination of 5 parcellations of a range of sizes from 100-1000, parcellations of size 100, 325, 550, 775 and 1000 could be used.
